version: '3'

# Memory resource allocation:
#
# - spark-master
# 64M - job_runner
# 256M - spark-daemon
# 512M - spark-driver (created per job)
#
# Number of concurrent jobs is equal to number of indicators.
# if max_job_num = 3: 512M * 3 = 1536M
#
# 320M + 1536M < 2500M
# 2500M + 500M (reservation)
#
#
# - spark-worker
# 256M - spark-daemon
# 2100M - memory for executors
#
# 2500M + 500M (reservation)
#
#
# - elasticsearch
# ???
#
#
# - hdfs
#
# ???
#
#
# TOTAL:
# 5000M + 500M (reservation) + elasticsearch + hdfs < 7000M (minimal memory for running installation)

services:

  # TODO run in distributed mode
  hdfs:
    build: ./img
    networks:
      - lab
    expose:
      - "9000"
    ports:
    # TODO remove exposing 9000 port
      - "9000:9000"
      - "50070:50070"
    command: >
      bash -c "service ssh start
      && hadoop namenode -format -force
      && /usr/local/hadoop/sbin/start-dfs.sh
      && tail -f /usr/local/hadoop/logs/*"

  spark-slave:
    build: ./img
    depends_on:
      - spark-master
    links:
      - hdfs
      - elasticsearch
    deploy:
      resources:
        limits:
          memory: 2500M
        reservations:
          memory: 500M
    networks:
      - lab
    ports:
      - "8081:8081"
    environment:
      - SPARK_WORKER_PORT=7999
      - SPARK_WORKER_WEBUI_PORT=8081
    command: >
      bash -c "service ssh start
      && /usr/local/spark/sbin/start-slave.sh spark://spark-master:7071
      && tail -f /usr/local/spark/logs/*"

  spark-master:
    build: ./img
    ports:
      - "8080:8080"
      - "5557:5557" # job_runner
    links:
      - hdfs
      - elasticsearch
    deploy:
      resources:
        limits:
          memory: 2500M
        reservations:
          memory: 500M
    networks:
      - lab
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7071
      - SPARK_MASTER_WEBUI_PORT=8080
      - HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop # for mahout
    command: >
      bash -c "service ssh start
      && /usr/local/spark/sbin/start-master.sh
      && java -Xmx64M -cp /usr/local/job_runner-assembly-0.1.jar lab.reco.job.WebServer"

    # && tail -f /usr/local/spark/logs/*"

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2
    container_name: elasticsearch
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "xpack.graph.enabled=false"
      - "xpack.ml.enabled=false"
      - "xpack.monitoring.enabled=false"
      - "xpack.security.enabled=false"
      - "xpack.watcher.enabled=false"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata1:/usr/share/elasticsearch/data
    expose:
      - "9200"
    ports:
      - "9200:9200"
    networks:
      - lab

#  elasticsearch2:
#    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2
#    container_name: elasticsearch2
#    environment:
#      - cluster.name=docker-cluster
#      - bootstrap.memory_lock=true
#      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
#      - "discovery.zen.ping.unicast.hosts=elasticsearch"
#      - "xpack.graph.enabled=false"
#      - "xpack.ml.enabled=false"
#      - "xpack.monitoring.enabled=false"
#      - "xpack.security.enabled=false"
#      - "xpack.watcher.enabled=false"
#    expose:
#      - "9200"
#    ulimits:
#      memlock:
#        soft: -1
#        hard: -1
#    volumes:
#      - esdata2:/usr/share/elasticsearch/data
#    networks:
#      - lab

volumes:
  esdata1:
    driver: local
  esdata2:
    driver: local

networks:
  lab:
