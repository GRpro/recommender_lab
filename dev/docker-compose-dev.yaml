version: '3'

services:

  # Memory expectations 300M
  event_manager:
    build: ./img
    depends_on:
      - elasticsearch
    networks:
      - lab
    expose:
      - "5555"
    ports:
      - "5555:5555"
    command: >
      bash -c "service ssh start
      && java -Xmx256M -Dconfig.file=/usr/local/event_manager/application.conf -cp /usr/local/event_manager/event_manager-assembly-0.1.jar lab.reco.event.WebServer"

  # Memory expectations 300M
  recommender:
    build: ./img
    depends_on:
      - elasticsearch
    networks:
      - lab
    expose:
      - "5556"
    ports:
      - "5556:5556"
    command: >
      bash -c "service ssh start
      && java -Xmx256M -Dconfig.file=/usr/local/recommender/application.conf -cp /usr/local/recommender/recommender-assembly-0.1.jar lab.reco.engine.WebServer"


  # TODO run in distributed mode
  hdfs:
    build: ./img
    networks:
      - lab
    expose:
      - "9000"
    ports:
      - "50070:50070"
    command: >
      bash -c "service ssh start
      && hadoop namenode -format -force
      && /usr/local/hadoop/sbin/start-dfs.sh
      && tail -f /usr/local/hadoop/logs/*"

  # Memory expectations 2500M
  # 256M - spark-daemon
  # 2048M - memory for executors
  spark-slave:
    build: ./img
    depends_on:
      - spark-master
    links:
      - hdfs
      - elasticsearch
    ports:
      - "8081-8082:8081" # Open UI ports for slaves (2 slaves)
    deploy:
      resources:
        limits:
          memory: 2500M
        reservations:
          memory: 2400M
    networks:
      - lab
    environment:
      - SPARK_WORKER_PORT=7999
      - SPARK_WORKER_WEBUI_PORT=8081
    command: >
      bash -c "service ssh start
      && /usr/local/spark/sbin/start-slave.sh spark://spark-master:7071
      && tail -f /usr/local/spark/logs/*"

  # Memory expectations 2000M
  # 64M - job_runner
  # 256M - spark-daemon
  # 1500M - spark-driver
  spark-master:
    build: ./img
    ports:
      - "8080:8080"
      - "4040:4040"
      - "5557:5557" # job_runner
    links:
      - hdfs
      - elasticsearch
    deploy:
      resources:
        limits:
          memory: 2000M
        reservations:
          memory: 1900M
    networks:
      - lab
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7071
      - SPARK_MASTER_WEBUI_PORT=8080
      - HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop # for mahout
    command: >
      bash -c "service ssh start
      && /usr/local/spark/sbin/start-master.sh
      && java -Xmx64M -Dconfig.file=/usr/local/job_runner/application.conf -cp /usr/local/job_runner/job_runner-assembly-0.1.jar lab.reco.job.WebServer"

  # Memory expectations 1100M
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2
    container_name: elasticsearch
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"
      - "xpack.graph.enabled=false"
      - "xpack.ml.enabled=false"
      - "xpack.monitoring.enabled=false"
      - "xpack.security.enabled=false"
      - "xpack.watcher.enabled=false"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata1:/usr/share/elasticsearch/data
    expose:
      - "9200"
    ports:
      - "9200:9200"
    networks:
      - lab

#  elasticsearch2:
#    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2
#    container_name: elasticsearch2
#    environment:
#      - cluster.name=docker-cluster
#      - bootstrap.memory_lock=true
#      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"
#      - "discovery.zen.ping.unicast.hosts=elasticsearch"
#      - "xpack.graph.enabled=false"
#      - "xpack.ml.enabled=false"
#      - "xpack.monitoring.enabled=false"
#      - "xpack.security.enabled=false"
#      - "xpack.watcher.enabled=false"
#    expose:
#      - "9200"
#    ulimits:
#      memlock:
#        soft: -1
#        hard: -1
#    volumes:
#      - esdata2:/usr/share/elasticsearch/data
#    networks:
#      - lab

volumes:
  esdata1:
    driver: local
  esdata2:
    driver: local

networks:
  lab:
